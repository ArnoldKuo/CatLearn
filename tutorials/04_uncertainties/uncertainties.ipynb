{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fourth CatLearn tutorial.\n",
    "\n",
    "This tutorial is intended to give further intuition for Gaussian processes.\n",
    "\n",
    "Like in tutorial 3, we set up a known underlying function with two training features and one output feature, we generate training and test data and calculate predictions and errors.\n",
    "\n",
    "We will compare the results of linear ridge regression, Gaussian linear kernel regression and finally a Gaussian process with the usual squared exponential kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "from catlearn.preprocess.scaling import standardize, target_standardize\n",
    "from catlearn.regression import GaussianProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A known underlying function in two dimensions\n",
    "def afunc(x):\n",
    "    \"\"\"2D linear function (plane).\"\"\"\n",
    "    return 3. * x[:, 0] - 1. * x[:, 1] + 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up data.\n",
    "# A number of training points in x.\n",
    "train_points = 30\n",
    "# Magnitude of the noise.\n",
    "noise_magnitude = 0.3\n",
    "\n",
    "# Randomly generate the training datapoints x.\n",
    "train_d1 = 2 * (np.random.random_sample(train_points) - 0.5)\n",
    "train_d2 = 2 * (np.random.random_sample(train_points) - 0.5)\n",
    "train_x1, train_x2 = np.meshgrid(train_d1, train_d2)\n",
    "train = np.hstack([np.vstack(train_d1), np.vstack(train_d2)])\n",
    "\n",
    "# Each element in the list train can be referred to as a fingerprint.\n",
    "# Call the underlying function to produce the target values.\n",
    "target = np.array(afunc(train))\n",
    "\n",
    "# Add random noise from a normal distribution to the target values.\n",
    "for i in range(train_points):\n",
    "    target[i] += noise_magnitude * np.random.normal()\n",
    "\n",
    "# Generate test datapoints x.\n",
    "test_points = 30\n",
    "test1d = np.vstack(np.linspace(-1.5, 1.5, test_points))\n",
    "test_x1, test_x2 = np.meshgrid(test1d, test1d)\n",
    "test = np.hstack([np.vstack(test_x1.ravel()), np.vstack(test_x2.ravel())])\n",
    "\n",
    "print(np.shape(train))\n",
    "print(np.shape(test))\n",
    "print(np.shape(target))\n",
    "\n",
    "# Standardize the training and test data on the same scale.\n",
    "std = standardize(train_matrix=train,\n",
    "                  test_matrix=test)\n",
    "# Standardize the training targets.\n",
    "train_targets = target_standardize(target)\n",
    "# Note that predictions will now be made on the standardized scale.\n",
    "\n",
    "# plot predicted tstd * uncertainties intervals.\n",
    "tstd = 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `tstd` variable specifies how many standard deviations we plot.\n",
    "\n",
    "# Model example 1 - Gausian linear kernel regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prediction parameters\n",
    "kdict = {'k1': {'type': 'linear', 'scaling': 1.},\n",
    "         'c1': {'type': 'constant', 'const': 0.}}\n",
    "# Starting guess for the noise parameter\n",
    "sdt1 = noise_magnitude\n",
    "# Set up the gaussian process.\n",
    "gp1 = GaussianProcess(kernel_dict=kdict, regularization=sdt1,\n",
    "                      train_fp=std['train'],\n",
    "                      train_target=train_targets['target'],\n",
    "                      optimize_hyperparameters=True)\n",
    "# Do predictions.\n",
    "linear = gp1.predict(test_fp=std['test'], uncertainty=True)\n",
    "# Put predictions back on real scale.\n",
    "prediction = np.vstack(linear['prediction']) * train_targets['std'] + \\\n",
    "    train_targets['mean']\n",
    "# Put uncertainties back on real scale.\n",
    "uncertainty = np.vstack(linear['uncertainty']) * train_targets['std']\n",
    "# Get confidence interval on predictions.\n",
    "over_upper = prediction + uncertainty * tstd\n",
    "over_lower = prediction - uncertainty * tstd\n",
    "# Plotting.\n",
    "plt3d = plt.figure().gca(projection='3d')\n",
    "\n",
    "# Plot training data.\n",
    "plt3d.scatter(train[:, 0], train[:, 1], target,  color='b')\n",
    "\n",
    "# Plot exact function.\n",
    "plt3d.plot_surface(test_x1, test_x2,\n",
    "                   afunc(test).reshape(np.shape(test_x1)),\n",
    "                   alpha=0.3, color='b')\n",
    "# Plot the uncertainties upper and lower bounds.\n",
    "plt3d.plot_surface(test_x1, test_x2,\n",
    "                   over_upper.reshape(np.shape(test_x1)),\n",
    "                   alpha=0.3, color='r')\n",
    "plt3d.plot_surface(test_x1, test_x2,\n",
    "                   over_lower.reshape(np.shape(test_x1)),\n",
    "                   alpha=0.3, color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the upper and lower bounds of the confidence interval predicted by the linear model. It is fairly confident on even beyond the region of the training data set.\n",
    "\n",
    "# Model example 2 - squared exponential kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the prediction routine and optimize hyperparameters.\n",
    "kdict = {'k1': {'type': 'gaussian', 'width': [0.3, 3.]}}\n",
    "# Starting guess for the noise parameter\n",
    "sdt1 = noise_magnitude\n",
    "# Set up the gaussian process.\n",
    "gp2 = GaussianProcess(kernel_dict=kdict, regularization=sdt1,\n",
    "                      train_fp=std['train'],\n",
    "                      train_target=train_targets['target'],\n",
    "                      optimize_hyperparameters=True)\n",
    "# Do the optimized predictions.\n",
    "gaussian = gp2.predict(test_fp=std['test'], uncertainty=True)\n",
    "# Put predictions back on real scale.\n",
    "prediction = np.vstack(gaussian['prediction']) * train_targets['std'] + \\\n",
    "    train_targets['mean']\n",
    "# Put uncertainties back on real scale.\n",
    "uncertainty = np.vstack(gaussian['uncertainty']) * train_targets['std']\n",
    "# Get confidence interval on predictions.\n",
    "gp_upper = prediction + uncertainty * tstd\n",
    "gp_lower = prediction - uncertainty * tstd\n",
    "# Plotting.\n",
    "plt3d = plt.figure().gca(projection='3d')\n",
    "\n",
    "# Plot training data.\n",
    "plt3d.scatter(train[:, 0], train[:, 1], target,  color='b')\n",
    "\n",
    "# Plot exact function.\n",
    "plt3d.plot_surface(test_x1, test_x2,\n",
    "                   afunc(test).reshape(np.shape(test_x1)),\n",
    "                   alpha=0.3, color='b')\n",
    "# Plot the prediction.\n",
    "plt3d.plot_surface(test_x1, test_x2,\n",
    "                   gp_upper.reshape(np.shape(test_x1)),\n",
    "                   alpha=0.3, color='g')\n",
    "plt3d.plot_surface(test_x1, test_x2,\n",
    "                   gp_lower.reshape(np.shape(test_x1)),\n",
    "                   alpha=0.3, color='g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see the confidence interval grows much faster, revealing that the squared exponential kernel is more uncertain outside the region of the training data.\n",
    "\n",
    "### Experiment and get intuition.\n",
    "\n",
    "Now, try playing around with the `train_points`, `noise_magnitude`  and `tstd` variables and rerun the models, to get a feel for the behavior of the Gaussian process and for viewing the various levels of confidence the two models can achieve. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
